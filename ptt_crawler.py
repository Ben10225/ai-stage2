import requests
from bs4 import BeautifulSoup
import time
import pandas as pd
import concurrent.futures
import os
import random
from requests.exceptions import RequestException


# è¨­ç½®çˆ¬èŸ²çš„åƒæ•¸
MAX_RETRIES = 5
MAX_ARTICLES = 200000
PTT_URL = "https://www.ptt.cc"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}
# PTT éƒ¨åˆ†çœ‹æ¿ï¼ˆå¦‚Gossipingï¼‰æœ‰ã€Œæ»¿18æ­²ã€é©—è­‰ï¼Œéœ€è¦å‚³éä¸€å€‹ cookie
COOKIES = {"over18": "1"}


def fetch_page_with_retry(url):
    """å˜—è©¦å¤šæ¬¡é€£ç·šï¼Œä»¥æ‡‰å°æš«æ™‚çš„ Connection Aborted éŒ¯èª¤"""
    for attempt in range(MAX_RETRIES):
        try:
            # è¨­ç½®è¶…æ™‚æ™‚é–“ï¼Œé¿å…ç„¡é™ç­‰å¾…
            response = requests.get(url, headers=HEADERS, cookies=COOKIES, timeout=15)

            # æª¢æŸ¥ HTTP ç‹€æ…‹ç¢¼ (4xx æˆ– 5xx éŒ¯èª¤)
            response.raise_for_status()

            # è«‹æ±‚æˆåŠŸï¼Œè¿”å›å›æ‡‰ç‰©ä»¶
            return response

        except RequestException as e:
            # æ•æ‰æ‰€æœ‰çš„ requests éŒ¯èª¤ï¼ŒåŒ…æ‹¬ Connection Aborted, Reset, Timeout, HTTPError ç­‰
            print(f"âŒ å˜—è©¦ {attempt + 1}/{MAX_RETRIES} å¤±æ•—: {url}ã€‚éŒ¯èª¤: {e}")

            if attempt < MAX_RETRIES - 1:
                # åŸ·è¡Œé‡è©¦å‰ï¼Œä½¿ç”¨éå¢çš„éš¨æ©Ÿå»¶é² (Exponential Backoff with Jitter)
                # é€™æ¨£èƒ½è®“è«‹æ±‚çœ‹èµ·ä¾†æ›´åˆ†æ•£ï¼Œä¸¦çµ¦ä¼ºæœå™¨æ›´å¤šæ¢å¾©æ™‚é–“ã€‚
                sleep_time = random.uniform(2**attempt, 2 ** (attempt + 1))
                # ç‚ºäº†é¿å…å¤ªä¹…ï¼Œå¯ä»¥è¨­å®šä¸€å€‹ä¸Šé™ï¼Œä¾‹å¦‚ 30 ç§’
                if sleep_time > 30:
                    sleep_time = random.uniform(20, 30)

                print(f"ç­‰å¾… {sleep_time:.2f} ç§’å¾Œé‡è©¦...")
                time.sleep(sleep_time)
            else:
                # é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ‹‹å‡ºéŒ¯èª¤çµ¦ä¸Šå±¤å‡½å¼è™•ç†
                print(f"ğŸš¨ é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„é é¢: {url}")
                raise

    return None  # ç†è«–ä¸Šä¸æœƒåŸ·è¡Œåˆ°é€™è£¡


def get_articles_from_page(url):
    # å¾å–®é é¢æŠ“å–æ–‡ç« æ¨™é¡Œå’Œä¸Šä¸€é çš„é€£çµ
    try:
        response = fetch_page_with_retry(url)

    except Exception as e:
        # å¦‚æœ fetch_page_with_retry åœ¨é”åˆ°æœ€å¤§æ¬¡æ•¸å¾Œä»ç„¶å¤±æ•—ï¼Œæœƒæ‹‹å‡ºç•°å¸¸
        print(f"è‡´å‘½éŒ¯èª¤ï¼Œç„¡æ³•æŠ“å–é é¢: {url}")
        return [], None

    soup = BeautifulSoup(response.text, "html.parser")
    articles = []

    # æ–‡ç« åˆ—è¡¨çš„å€å¡Š
    for div in soup.find_all("div", class_="r-ent"):
        # æŠ“å–æ–‡ç« æ¨™é¡Œ
        title_tag = div.find("div", class_="title").find("a")

        # æ’é™¤è¢«åˆªé™¤æˆ–ç„¡æ¨™é¡Œçš„æ–‡ç«  (æ¨™é¡Œtagç‚ºNone)
        if title_tag:
            title = title_tag.text.strip()
            articles.append(title)

    # å°‹æ‰¾ã€Œä¸Šä¸€é ã€çš„é€£çµ (åœ¨ PTT ç¶²é ä¸Šæ˜¯ã€Œä¸Šé ã€)
    paging_div = soup.find("div", class_="btn-group btn-group-paging")
    prev_page_link = None
    if paging_div:
        prev_button = paging_div.find("a", string="â€¹ ä¸Šé ")

        if "href" in prev_button.attrs:
            prev_page_link = PTT_URL + prev_button["href"]

    time.sleep(random.uniform(0.5, 1.5))

    return articles, prev_page_link


def crawl_board(initial_url, board_name):
    """éè¿´æŠ“å–å¤šé æ–‡ç« æ¨™é¡Œï¼Œç›´åˆ°é”åˆ°æœ€å¤§æ•¸é‡æˆ–æ²’æœ‰ä¸Šä¸€é """
    print(f"--- é–‹å§‹çˆ¬å–çœ‹æ¿: {board_name} ---")
    current_url = initial_url
    all_titles = []

    page_count = 0

    while current_url and len(all_titles) < MAX_ARTICLES:
        print(f"çˆ¬å–é é¢: {current_url}")

        articles_on_page, next_url = get_articles_from_page(current_url)
        page_count += 1

        # å¢åŠ çœ‹æ¿æ¨™ç±¤
        newly_fetched = 0
        for title in articles_on_page:
            if len(all_titles) < MAX_ARTICLES:
                all_titles.append((title, board_name))
                newly_fetched += 1
            else:
                break

        current_total = len(all_titles)
        print(
            f"[{board_name}] é é¢ {page_count} çˆ¬å–å®Œæˆã€‚æœ¬æ¬¡æ–°å¢: {newly_fetched} ç¯‡ã€‚ç´¯ç©ç¸½æ•¸: {current_total} ç¯‡ / ç›®æ¨™ {MAX_ARTICLES}"
        )

        # å¦‚æœé”åˆ°ä¸Šé™ï¼Œå‰‡åœæ­¢
        if len(all_titles) >= MAX_ARTICLES:
            break

        current_url = next_url
        time.sleep(random.uniform(1.5, 2.5))

    # å„²å­˜ç‚º CSV
    filename = f"{board_name}.csv"
    df = pd.DataFrame(all_titles, columns=["Title", "Board"])

    BASE_DIR = "csv/raw"
    os.makedirs(BASE_DIR, exist_ok=True)
    full_path = os.path.join(BASE_DIR, filename)

    df.to_csv(full_path, index=False, encoding="utf-8-sig")
    print(
        f"çœ‹æ¿ {board_name} çˆ¬å–å®Œæˆï¼Œå…± {len(all_titles)} ç¯‡æ–‡ç« ï¼Œå·²ä¿å­˜è‡³ {filename}"
    )
    print("-" * 30)


def run_concurrently(board_list):
    MAX_WORKERS = 3

    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # ä½¿ç”¨ submit å°‡æ¯å€‹ crawl_board ä»»å‹™æäº¤çµ¦åŸ·è¡Œç·’æ± 
        future_to_board = {
            executor.submit(crawl_board, url, name): name
            for name, url in board_list.items()
        }

        # å¯é¸ï¼šç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆä¸¦è™•ç†çµæœ/éŒ¯èª¤
        for future in concurrent.futures.as_completed(future_to_board):
            board_name = future_to_board[future]
            try:
                # ç²å– crawl_board çš„è¿”å›çµæœ (å¦‚æœæœ‰çš„è©±)
                data = future.result()
                print(f"çœ‹æ¿ {board_name} å·²å®Œæˆä¸¦è¡Œçˆ¬å–ã€‚")
            except Exception as exc:
                print(f"çœ‹æ¿ {board_name} åœ¨çˆ¬å–æ™‚ç™¼ç”ŸéŒ¯èª¤: {exc}")


board_list = {
    "baseball": "https://www.ptt.cc/bbs/Baseball/index.html",
    "boy_girl": "https://www.ptt.cc/bbs/Boy-Girl/index.html",
    "c_chat": "https://www.ptt.cc/bbs/c_chat/index.html",
    "hate_politics": "https://www.ptt.cc/bbs/hatepolitics/index.html",
    "life_is_money": "https://www.ptt.cc/bbs/Lifeismoney/index.html",
    "military": "https://www.ptt.cc/bbs/Military/index.html",
    "pc_shopping": "https://www.ptt.cc/bbs/pc_shopping/index.html",
    "stock": "https://www.ptt.cc/bbs/stock/index.html",
    "tech_job": "https://www.ptt.cc/bbs/Tech_Job/index.html",
}

if __name__ == "__main__":
    run_concurrently(board_list)
