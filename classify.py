import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
from gensim.models.doc2vec import Doc2Vec
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import os

# ç¢ºä¿ models è³‡æ–™å¤¾å­˜åœ¨
save_dir = "models"
if not os.path.exists(save_dir):
    os.makedirs(save_dir)

# å„²å­˜è·¯å¾‘
save_path = os.path.join(save_dir, "classifier_88plus.pth")

# --- 1. è³‡æ–™æº–å‚™ ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è¨­å‚™: {device}")

df = pd.read_csv("csv/tokenized/shuffled_articles.csv")
d2v_model = Doc2Vec.load("models/doc2vec.model")

# æ¨™ç±¤ç·¨ç¢¼
le = LabelEncoder()
df["label"] = le.fit_transform(df["Board"])
num_classes = len(le.classes_)

# æº–å‚™ X (å‘é‡) å’Œ y (æ¨™ç±¤)
X = np.array([d2v_model.dv[i] for i in range(len(df))])
y = df["label"].values

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# --- 2. å»ºç«‹ PyTorch Dataset ---
class ArticleDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.long)

    def __len__(self):
        return len(self.y)

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


train_loader = DataLoader(ArticleDataset(X_train, y_train), batch_size=32, shuffle=True)
test_loader = DataLoader(ArticleDataset(X_test, y_test), batch_size=32)


# --- 3. å®šç¾© MLP æ¨¡å‹ ---
class MultiClassClassification(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MultiClassClassification, self).__init__()
        self.net = nn.Sequential(
            # è¼¸å…¥å±¤ -> éš±è—å±¤ 1
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            # éš±è—å±¤ 1 -> éš±è—å±¤ 2
            nn.Linear(hidden_dim, 32),
            nn.ReLU(),
            # éš±è—å±¤ 2 -> è¼¸å‡ºå±¤
            nn.Linear(32, output_dim),
            # æ³¨æ„ï¼šè¨“ç·´æ™‚é€™è£¡ã€Œä¸æ”¾ã€Softmaxï¼Œç›´æ¥è¼¸å‡º Raw Scores (Logits)
        )

    def forward(self, x):
        return self.net(x)


# åˆå§‹åŒ–æ¨¡å‹
# input_dim = 70 (Doc2Vec), hidden_dim å»ºè­° 64~128, output_dim ç‚ºçœ‹æ¿ç¸½æ•¸
model = MultiClassClassification(input_dim=150, hidden_dim=128, output_dim=num_classes)

# Categorical Cross Entropy åœ¨ PyTorch ä¸­å°±æ˜¯ CrossEntropyLoss
# å®ƒæœƒè‡ªå‹•å¹«ä½ çš„è¼¸å‡ºåš Softmax + Log + NLLLoss
criterion = nn.CrossEntropyLoss()

# å„ªåŒ–å™¨å¸¸ç”¨ Adam
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

# ç•¶ Acc ä¸‰å€‹ Epoch æ²’é€²æ­¥ï¼Œå°±æŠŠ LR é™¤ä»¥ 2
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, "max", patience=3, factor=0.5
)

# --- 5. è¨“ç·´è¿´åœˆ ---

target_accuracy = 85
epochs = 30  # æŠŠä¸Šé™è¨­é«˜ï¼Œè®“å®ƒæœ‰æ™‚é–“è¡åˆº

for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch_X, batch_y in train_loader:
        batch_X, batch_y = batch_X.to(device), batch_y.to(device)

        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    # é©—è­‰éšæ®µ
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for batch_X, batch_y in test_loader:
            outputs = model(batch_X)
            _, predicted = torch.max(outputs.data, 1)
            total += batch_y.size(0)
            correct += (predicted == batch_y).sum().item()

    current_acc = 100 * correct / total
    print(
        f"Epoch [{epoch+1}], Loss: {total_loss/len(train_loader):.4f}, Acc: {current_acc:.2f}%"
    )

    # Scheduler æœƒæ ¹æ“š current_acc æ˜¯å¦ä¸å†é€²æ­¥ï¼Œä¾†æ±ºå®šè¦ä¸è¦èª¿é™ LR
    scheduler.step(current_acc)

    # æª¢æŸ¥æ˜¯å¦é”åˆ°ç›®æ¨™
    if current_acc >= target_accuracy:
        print(f"ğŸ‰ é”åˆ°ç›®æ¨™æº–ç¢ºç‡ {target_accuracy}% åœæ­¢è¨“ç·´")
        torch.save(model.state_dict(), save_path)
        break
